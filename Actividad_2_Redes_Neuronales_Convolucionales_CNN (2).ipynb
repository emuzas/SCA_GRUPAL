{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8gZlVyCCbz"
      },
      "source": [
        "# ACTIVIDAD 2: REDES NEURONALES CONVOLUCIONALES\n",
        "\n",
        "---\n",
        "\n",
        "En esta actividad, vamos a trabajar con Convolutional Neural Networks para resolver un problema de clasificación de imágenes. En particular, vamos a clasificar imágenes de personajes de la conocida serie de los Simpsons.\n",
        "\n",
        "Como las CNN profundas son un tipo de modelo bastante avanzado y computacionalmente costoso, se recomienda hacer la práctica en Google Colaboratory con soporte para GPUs. En [este enlace](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) se explica cómo activar un entorno con GPUs. *Nota: para leer las imágenes y estandarizarlas al mismo tamaño se usa la librería opencv. Esta ĺibrería está ya instalada en el entorno de Colab, pero si trabajáis de manera local tendréis que instalarla.*\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/i8zIGqX.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
        "\n",
        "El dataset a utilizar consiste en imágenes de personajes de los Simpsons extraídas directamente de capítulos de la serie. Este dataset ha sido recopilado por [Alexandre Attia](http://www.alexattia.fr/) y es más complejo que el dataset de Fashion MNIST que hemos utilizado hasta ahora. Aparte de tener más clases (vamos a utilizar los 18 personajes con más imágenes), los personajes pueden aparecer en distintas poses, en distintas posiciones de la imagen o con otros personajes en pantalla (si bien el personaje a clasificar siempre aparece en la posición predominante).\n",
        "\n",
        "El dataset de training puede ser descargado desde aquí:\n",
        "\n",
        "[Training data](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219337&authkey=AMzI92bJPx8Sd60) (~500MB)\n",
        "\n",
        "Por otro lado, el dataset de test puede ser descargado de aquí:\n",
        "\n",
        "[Test data](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219341&authkey=ANnjK3Uq1FhuAe8) (~10MB)\n",
        "\n",
        "Antes de empezar la práctica, se recomienda descargar las imágenes y echarlas un vistazo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI274F8LQC59"
      },
      "source": [
        "## Carga de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D7tKOZ9BFfki"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np \n",
        "import keras\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iw0apA7ruy1c",
        "outputId": "d000ba7a-8565-4095-b04a-dc7795613963",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219337&authkey=AMzI92bJPx8Sd60\n",
            "523789527/523789527 [==============================] - 24s 0us/step\n",
            "Downloading data from https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219341&authkey=ANnjK3Uq1FhuAe8\n",
            "10658925/10658925 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Primero, bajamos los datos de entrenamiento\n",
        "keras.utils.get_file(fname=\"simpsons_train.tar.gz\", \n",
        "                     origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219337&authkey=AMzI92bJPx8Sd60\")\n",
        "\n",
        "# Descomprimimos el archivo\n",
        "!tar -xzf /root/.keras/datasets/simpsons_train.tar.gz -C /root/.keras/datasets\n",
        "\n",
        "\n",
        "# Hacemos lo mismo con los datos de test\n",
        "keras.utils.get_file(fname=\"simpsons_test.tar.gz\", \n",
        "                     origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219341&authkey=ANnjK3Uq1FhuAe8\")\n",
        "!tar -xzf /root/.keras/datasets/simpsons_test.tar.gz -C /root/.keras/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_HIhp512sPUJ"
      },
      "outputs": [],
      "source": [
        "# Descomprimimos el archivo en tmp para visualizar\n",
        "# !tar -xzf /root/.keras/datasets/simpsons_train.tar.gz -C /tmp/simpsons"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Q4VaiHIQUO6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hMFhe3COFwSD"
      },
      "outputs": [],
      "source": [
        "# Esta variable contiene un mapeo de número de clase a personaje.\n",
        "# Utilizamos sólo los 18 personajes del dataset que tienen más imágenes.\n",
        "MAP_CHARACTERS = {\n",
        "    0: 'abraham_grampa_simpson', 1: 'apu_nahasapeemapetilon', 2: 'bart_simpson',\n",
        "    3: 'charles_montgomery_burns', 4: 'chief_wiggum', 5: 'comic_book_guy', 6: 'edna_krabappel', \n",
        "    7: 'homer_simpson', 8: 'kent_brockman', 9: 'krusty_the_clown', 10: 'lisa_simpson', \n",
        "    11: 'marge_simpson', 12: 'milhouse_van_houten', 13: 'moe_szyslak', \n",
        "    14: 'ned_flanders', 15: 'nelson_muntz', 16: 'principal_skinner', 17: 'sideshow_bob'\n",
        "}\n",
        "\n",
        "# Vamos a standarizar todas las imágenes a tamaño 64x64\n",
        "IMG_SIZE = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5bJ0NsbCbupF"
      },
      "outputs": [],
      "source": [
        "def load_train_set(dirname, map_characters, verbose=True):\n",
        "    \"\"\"Esta función carga los datos de training en imágenes.\n",
        "    \n",
        "    Como las imágenes tienen tamaños distintas, utilizamos la librería opencv\n",
        "    para hacer un resize y adaptarlas todas a tamaño IMG_SIZE x IMG_SIZE.\n",
        "    \n",
        "    Args:\n",
        "        dirname: directorio completo del que leer los datos\n",
        "        map_characters: variable de mapeo entre labels y personajes\n",
        "        verbose: si es True, muestra información de las imágenes cargadas\n",
        "     \n",
        "    Returns:\n",
        "        X, y: X es un array con todas las imágenes cargadas con tamaño\n",
        "                IMG_SIZE x IMG_SIZE\n",
        "              y es un array con las labels de correspondientes a cada imagen\n",
        "    \"\"\"\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for label, character in map_characters.items():        \n",
        "        files = os.listdir(os.path.join(dirname, character))\n",
        "        images = [file for file in files if file.endswith(\"jpg\")]\n",
        "        if verbose:\n",
        "          print(\"Leyendo {} imágenes encontradas de {}\".format(len(images), character))\n",
        "        for image_name in images:\n",
        "            image = cv2.imread(os.path.join(dirname, character, image_name))\n",
        "            X_train.append(cv2.resize(image,(IMG_SIZE, IMG_SIZE)))\n",
        "            y_train.append(label)\n",
        "    return np.array(X_train), np.array(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NslxhnnDK6uA"
      },
      "outputs": [],
      "source": [
        "def load_test_set(dirname, map_characters, verbose=True):\n",
        "    \"\"\"Esta función funciona de manera equivalente a la función load_train_set\n",
        "    pero cargando los datos de test.\"\"\"\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "    reverse_dict = {v: k for k, v in map_characters.items()}\n",
        "    for filename in glob.glob(dirname + '/*.*'):\n",
        "        char_name = \"_\".join(filename.split('/')[-1].split('_')[:-1])\n",
        "        if char_name in reverse_dict:\n",
        "            image = cv2.imread(filename)\n",
        "            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "            X_test.append(image)\n",
        "            y_test.append(reverse_dict[char_name])\n",
        "    if verbose:\n",
        "        print(\"Leídas {} imágenes de test\".format(len(X_test)))\n",
        "    return np.array(X_test), np.array(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVWqKxFcbwTu",
        "outputId": "ddc73ec4-fea3-4940-fbc3-c259b90758e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leyendo 913 imágenes encontradas de abraham_grampa_simpson\n",
            "Leyendo 623 imágenes encontradas de apu_nahasapeemapetilon\n",
            "Leyendo 1342 imágenes encontradas de bart_simpson\n",
            "Leyendo 1193 imágenes encontradas de charles_montgomery_burns\n",
            "Leyendo 986 imágenes encontradas de chief_wiggum\n",
            "Leyendo 469 imágenes encontradas de comic_book_guy\n",
            "Leyendo 457 imágenes encontradas de edna_krabappel\n",
            "Leyendo 2246 imágenes encontradas de homer_simpson\n",
            "Leyendo 498 imágenes encontradas de kent_brockman\n",
            "Leyendo 1206 imágenes encontradas de krusty_the_clown\n",
            "Leyendo 1354 imágenes encontradas de lisa_simpson\n",
            "Leyendo 1291 imágenes encontradas de marge_simpson\n",
            "Leyendo 1079 imágenes encontradas de milhouse_van_houten\n",
            "Leyendo 1452 imágenes encontradas de moe_szyslak\n",
            "Leyendo 1454 imágenes encontradas de ned_flanders\n",
            "Leyendo 358 imágenes encontradas de nelson_muntz\n",
            "Leyendo 1194 imágenes encontradas de principal_skinner\n",
            "Leyendo 877 imágenes encontradas de sideshow_bob\n",
            "Leídas 890 imágenes de test\n"
          ]
        }
      ],
      "source": [
        "# Cargamos los datos. Si no estás trabajando en colab, cambia los paths por\n",
        "# los de los ficheros donde hayas descargado los datos.\n",
        "DATASET_TRAIN_PATH_COLAB = \"/root/.keras/datasets/simpsons\"\n",
        "DATASET_TEST_PATH_COLAB = \"/root/.keras/datasets/simpsons_testset\"\n",
        "\n",
        "X, y = load_train_set(DATASET_TRAIN_PATH_COLAB, MAP_CHARACTERS)\n",
        "X_t, y_t = load_test_set(DATASET_TEST_PATH_COLAB, MAP_CHARACTERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2GY4vTFyfffv"
      },
      "outputs": [],
      "source": [
        "# Vamos a barajar aleatoriamente los datos. Esto es importante ya que si no\n",
        "# lo hacemos y, por ejemplo, cogemos el 20% de los datos finales como validation\n",
        "# set, estaremos utilizando solo un pequeño número de personajes, ya que\n",
        "# las imágenes se leen secuencialmente personaje a personaje.\n",
        "perm = np.random.permutation(len(X))\n",
        "X, y = X[perm], y[perm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "IOoQ7_0GrylF",
        "outputId": "d78556a0-031e-4231-e1e9-e86e54c06501"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f53f728ea10>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(X_t[2]) # recordad que siempre es preferible trabajar en blanco y negro\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBbmz9DMhVhc"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "Utilizando Convolutional Neural Networks con Keras, entrenar un clasificador que sea capaz de reconocer personajes en imágenes de los Simpsons con una accuracy en el dataset de test de, al menos, **85%**. Redactar un informe analizando varias de las alternativas probadas y los resultados obtenidos.\n",
        "\n",
        "A continuación se detallan una serie de aspectos orientativos que podrían ser analizados en vuestro informe (no es necesario tratar todos ellos ni mucho menos, esto son ideas orientativas de aspectos que podéis explorar):\n",
        "\n",
        "*   Análisis de los datos a utilizar.\n",
        "*   Análisis de resultados, obtención de métricas de *precision* y *recall* por clase y análisis de qué clases obtienen mejores o peores resultados.\n",
        "*   Análisis visual de los errores de la red. ¿Qué tipo de imágenes o qué personajes dan más problemas a nuestro modelo?\n",
        "*   Comparación de modelos CNNs con un modelo de Fully Connected para este problema.\n",
        "*   Utilización de distintas arquitecturas CNNs, comentando aspectos como su profundidad, hiperparámetros utilizados, optimizador, uso de técnicas de regularización, *batch normalization*, etc.\n",
        "*   [ *algo más difícil* ] Utilización de *data augmentation*. Esto puede conseguirse con la clase [ImageDataGenerator](https://keras.io/preprocessing/image/#imagedatagenerator-class) de Keras.\n",
        "\n",
        "Notas: \n",
        "* Recuerda partir los datos en training/validation para tener una buena estimación de los valores que nuestro modelo tendrá en los datos de test, así como comprobar que no estamos cayendo en overfitting. Una posible partición puede ser 80 / 20.\n",
        "* No es necesario mostrar en el notebook las trazas de entrenamiento de todos los modelos entrenados, si bien una buena idea seria guardar gráficas de esos entrenamientos para el análisis. Sin embargo, **se debe mostrar el entrenamiento completo del mejor modelo obtenido y la evaluación de los datos de test con este modelo**.\n",
        "* Las imágenes **no están normalizadas**. Hay que normalizarlas como hemos hecho en trabajos anteriores.\n",
        "* El test set del problema tiene imágenes un poco más \"fáciles\", por lo que es posible encontrarse con métricas en el test set bastante mejores que en el training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xboXggLENqMB"
      },
      "source": [
        "Se normalizan los valores de los datasets de training y test para que estén entre 0 y 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PyqZAqGTrkTL"
      },
      "outputs": [],
      "source": [
        "X  = X / 255.0\n",
        "X_t= X_t / 255.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbhlWv_sNqMC"
      },
      "source": [
        "Se proporciona información de los datos a utilizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "A-XZdD0juRQM",
        "outputId": "e071cbbd-3f11-4686-c9cd-2238ab640053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de imágenes de training: 18992\n",
            "Número de imágenes de test: 890\n",
            "El tamaño de las imágenes es: 60 x 60\n"
          ]
        }
      ],
      "source": [
        "#número de imágenes training\n",
        "print(f\"Número de imágenes de training: {X.shape[0]}\")\n",
        "#número de imágenes test\n",
        "print(f\"Número de imágenes de test: {X_t.shape[0]}\")\n",
        "# Se obtiene el tamaño de una imagen de entrenamiento\n",
        "print(f\"El tamaño de las imágenes es: {X.shape[1]} x {X.shape[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b03rX-p0NqMD"
      },
      "source": [
        "Se crea el dataset de validación en una proporción 80:20 respecto al de training original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6AEVl-3quRSt",
        "outputId": "082b1b72-ecc8-4dfd-9c03-67d69937f87e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de imágenes de training: 15193\n",
            "Número de imágenes de validación: 3799\n"
          ]
        }
      ],
      "source": [
        "#Dividimos el juego de datos de train, en 80% para entrenar el modelo y 20% para validarlo\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Número de imágenes de training: {train_images.shape[0]}\")\n",
        "#número de imágenes validación\n",
        "print(f\"Número de imágenes de validación: {val_images.shape[0]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiVTj7cPNqME"
      },
      "source": [
        "Se plantea el primer modelo de CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "i4Apu3KDNqMF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50, VGG16, VGG19, InceptionResNetV2\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, Activation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "img_height = 60\n",
        "img_width = 60\n",
        "batch_size = 32\n",
        "val_split = 0.2\n",
        "seed = 42\n",
        "num_classes = 4\n",
        "learning_rate = 0.001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzlUCXOrNqMF"
      },
      "source": [
        "Se pasan los labels a variables categóricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "duMLtARANqMG",
        "outputId": "5a050111-d9fb-4ef1-9d62-b670c622550a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15193\n",
            "3799\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "training_labels = to_categorical(train_labels, num_classes=18)\n",
        "print(len(training_labels))\n",
        "validation_labels = to_categorical(val_labels, num_classes=18)\n",
        "print(len(val_labels))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Añadimos una Callback personalizada que nos va a permitir detener el entrenamiento."
      ],
      "metadata": {
        "id": "GA2y_xvD5RNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallbackFinal(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('val_accuracy')> 0.96):\n",
        "              print(\"\\nAlcanzado el 96% de precisión en datos de test, se cancela el entrenamiento!!\")\n",
        "              self.model.stop_training = True\n",
        "\n",
        "callbackFinal = myCallbackFinal()"
      ],
      "metadata": {
        "id": "Jn4K5ZJv5P8c"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdNVUoWgNqMG"
      },
      "source": [
        "Se construye el primer modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EAFy3MOINqMH"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(60, 60, 3)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        \n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        \n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(4096, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(2048, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(1024, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),        \n",
        "        Dense(18, activation='softmax')  # 4 clases: sanas, enfermedad1, enfermedad2, enfermedad3\n",
        "     ])\n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HBf00U7NqMH"
      },
      "source": [
        "Se entrena el primer modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USHxvnZNNqMI",
        "outputId": "21854b79-a9c9-4b93-f113-7a43f6b6b11b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 60, 60, 64)        1792      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 60, 60, 64)       256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 30, 30, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 30, 30, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 30, 30, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 15, 15, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 15, 15, 256)       295168    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 15, 15, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 7, 7, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 7, 7, 512)         1180160   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 7, 7, 512)        2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 3, 3, 512)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4608)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              18878464  \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 4096)             16384     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2048)              8390656   \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 2048)             8192      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              2098176   \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 18)                18450     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,969,234\n",
            "Trainable params: 30,952,978\n",
            "Non-trainable params: 16,256\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/40\n",
            "475/475 [==============================] - 851s 2s/step - loss: 3.6238 - accuracy: 0.1445 - val_loss: 2.4925 - val_accuracy: 0.2706\n",
            "Epoch 2/40\n",
            "475/475 [==============================] - 849s 2s/step - loss: 2.8178 - accuracy: 0.2820 - val_loss: 1.8116 - val_accuracy: 0.4772\n",
            "Epoch 3/40\n",
            "475/475 [==============================] - 850s 2s/step - loss: 2.4747 - accuracy: 0.3561 - val_loss: 1.5176 - val_accuracy: 0.5478\n",
            "Epoch 4/40\n",
            "475/475 [==============================] - 854s 2s/step - loss: 2.1954 - accuracy: 0.4168 - val_loss: 1.3807 - val_accuracy: 0.5883\n",
            "Epoch 5/40\n",
            "475/475 [==============================] - 850s 2s/step - loss: 1.9822 - accuracy: 0.4664 - val_loss: 1.3400 - val_accuracy: 0.6073\n",
            "Epoch 6/40\n",
            "475/475 [==============================] - 845s 2s/step - loss: 1.8287 - accuracy: 0.5037 - val_loss: 1.2646 - val_accuracy: 0.6281\n",
            "Epoch 7/40\n",
            "475/475 [==============================] - 795s 2s/step - loss: 1.6930 - accuracy: 0.5308 - val_loss: 1.1553 - val_accuracy: 0.6670\n",
            "Epoch 8/40\n",
            "475/475 [==============================] - 839s 2s/step - loss: 1.5813 - accuracy: 0.5589 - val_loss: 1.1033 - val_accuracy: 0.6778\n",
            "Epoch 9/40\n",
            "475/475 [==============================] - 835s 2s/step - loss: 1.4780 - accuracy: 0.5842 - val_loss: 1.0472 - val_accuracy: 0.6954\n",
            "Epoch 10/40\n",
            "475/475 [==============================] - 818s 2s/step - loss: 1.3764 - accuracy: 0.6039 - val_loss: 0.9938 - val_accuracy: 0.7041\n",
            "Epoch 11/40\n",
            "475/475 [==============================] - 821s 2s/step - loss: 1.2946 - accuracy: 0.6314 - val_loss: 0.9455 - val_accuracy: 0.7297\n",
            "Epoch 12/40\n",
            "475/475 [==============================] - 817s 2s/step - loss: 1.2050 - accuracy: 0.6505 - val_loss: 0.8815 - val_accuracy: 0.7439\n",
            "Epoch 13/40\n",
            "475/475 [==============================] - 833s 2s/step - loss: 1.1274 - accuracy: 0.6686 - val_loss: 0.8865 - val_accuracy: 0.7412\n",
            "Epoch 14/40\n",
            "475/475 [==============================] - 832s 2s/step - loss: 1.0703 - accuracy: 0.6834 - val_loss: 0.9208 - val_accuracy: 0.7257\n",
            "Epoch 15/40\n",
            "475/475 [==============================] - 835s 2s/step - loss: 0.9984 - accuracy: 0.6995 - val_loss: 0.8657 - val_accuracy: 0.7478\n",
            "Epoch 16/40\n",
            "475/475 [==============================] - 839s 2s/step - loss: 0.9609 - accuracy: 0.7133 - val_loss: 0.8223 - val_accuracy: 0.7644\n",
            "Epoch 17/40\n",
            "475/475 [==============================] - 848s 2s/step - loss: 0.9030 - accuracy: 0.7276 - val_loss: 0.7427 - val_accuracy: 0.7849\n",
            "Epoch 18/40\n",
            "475/475 [==============================] - 842s 2s/step - loss: 0.8421 - accuracy: 0.7479 - val_loss: 0.7440 - val_accuracy: 0.7815\n",
            "Epoch 19/40\n",
            "475/475 [==============================] - 832s 2s/step - loss: 0.8122 - accuracy: 0.7520 - val_loss: 0.7245 - val_accuracy: 0.7944\n",
            "Epoch 20/40\n",
            "475/475 [==============================] - 844s 2s/step - loss: 0.7635 - accuracy: 0.7664 - val_loss: 0.6791 - val_accuracy: 0.7978\n",
            "Epoch 21/40\n",
            "475/475 [==============================] - 845s 2s/step - loss: 0.7318 - accuracy: 0.7749 - val_loss: 0.6839 - val_accuracy: 0.8036\n",
            "Epoch 22/40\n",
            "475/475 [==============================] - 855s 2s/step - loss: 0.6827 - accuracy: 0.7925 - val_loss: 0.6323 - val_accuracy: 0.8186\n",
            "Epoch 23/40\n",
            "475/475 [==============================] - 845s 2s/step - loss: 0.6593 - accuracy: 0.7931 - val_loss: 0.6320 - val_accuracy: 0.8178\n",
            "Epoch 24/40\n",
            "475/475 [==============================] - 841s 2s/step - loss: 0.6230 - accuracy: 0.8064 - val_loss: 0.6224 - val_accuracy: 0.8192\n",
            "Epoch 25/40\n",
            "475/475 [==============================] - 842s 2s/step - loss: 0.6013 - accuracy: 0.8164 - val_loss: 0.6047 - val_accuracy: 0.8244\n",
            "Epoch 26/40\n",
            "475/475 [==============================] - 854s 2s/step - loss: 0.5696 - accuracy: 0.8244 - val_loss: 0.5772 - val_accuracy: 0.8307\n",
            "Epoch 27/40\n",
            "475/475 [==============================] - 853s 2s/step - loss: 0.5423 - accuracy: 0.8320 - val_loss: 0.5951 - val_accuracy: 0.8307\n",
            "Epoch 28/40\n",
            "409/475 [========================>.....] - ETA: 1:47 - loss: 0.5107 - accuracy: 0.8388"
          ]
        }
      ],
      "source": [
        "model = build_model()\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "history = model.fit(train_images, training_labels, epochs=40, batch_size=32, validation_data=(val_images, validation_labels), callbacks=[callbackFinal])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwzf4PxYNqMI"
      },
      "source": [
        "Se evalua el modelo con los datos de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-ILt1KfNqMI"
      },
      "outputs": [],
      "source": [
        "test_labels = to_categorical(y_t, num_classes=18)\n",
        "test_loss, test_accuracy = model.evaluate(X_t, test_labels)\n",
        "print(\"Test Loss: \", test_loss)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT-rNDZoNqMJ"
      },
      "source": [
        "Se crea la matriz de confusión y el classification report para ver como se han comportado cada una de las clases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PFAPfWVNqMJ"
      },
      "outputs": [],
      "source": [
        "class_names = [MAP_CHARACTERS[i] for i in range(len(MAP_CHARACTERS))]\n",
        "\n",
        "# Matriz de confusión y métricas adicionales\n",
        "Y_pred = model.predict(X_t)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_t, y_pred, target_names=class_names))\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_t, y_pred))\n",
        "\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizamos un segundo modelo,. con un menor número de capas:"
      ],
      "metadata": {
        "id": "7jY_bl6cvypv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_example = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(60, 60, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    #tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    #tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(18, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "model_example.summary()\n",
        "model_example.compile(loss = 'categorical_crossentropy', optimizer=SGD(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "history = model_example.fit(train_images, training_labels, epochs=40, batch_size=32, validation_data=(val_images, validation_labels), callbacks=[callbackFinal])"
      ],
      "metadata": {
        "id": "nqNqTfnov28H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = to_categorical(y_t, num_classes=18)\n",
        "test_loss, test_accuracy = model_example.evaluate(X_t, test_labels)\n",
        "print(\"Test Loss: \", test_loss)\n",
        "print(\"Test Accuracy: \", test_accuracy)\n",
        "\n",
        "class_names = [MAP_CHARACTERS[i] for i in range(len(MAP_CHARACTERS))]\n",
        "\n",
        "# Matriz de confusión y métricas adicionales\n",
        "Y_pred = model_example.predict(X_t)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_t, y_pred, target_names=class_names))\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_t, y_pred))\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pekP8bFiwQBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver como utilizando un modelo más \"simple\" se obtienen mejores resultados, es decir, (citando la corrección de la actividad 1): Es necesario comprender que en Inteligencia Artificial el 100% no existe prácticamente en ningún caso (salvo en casos ideales) y que, en muchas ocasiones, no por utilizar las cosas más complejas se obtienen los mejores resultados (aunque en otras sí). Una capa oculta de 150 neuronas con una leaky rely (para que no mueran muchas neuronas) es suficiente para obtener este resultado."
      ],
      "metadata": {
        "id": "fCaCimD43CuB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKlcxVcSNqMJ"
      },
      "source": [
        "Se crea un segundo modelo, en este caso de Transfer Learning usando como base la red pre-etrenada VGG19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnCgu_4ZNqMJ"
      },
      "outputs": [],
      "source": [
        "def build_modelVGG19():\n",
        "    # Cargar la red pre-entrenada VGG19\n",
        "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "    \n",
        "    # Congelar todas las capas de la red base excepto las últimas 12\n",
        "    for layer in base_model.layers[:-12]:\n",
        "        layer.trainable = False\n",
        "        \n",
        "    # Añadir capas adicionales para la clasificación\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(18, activation='softmax')(x)\n",
        "\n",
        "    \n",
        "    \n",
        "    # Crear el modelo final\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    \n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKtpl_SVNqMK"
      },
      "source": [
        "Se entrena el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIgRFskYNqMK"
      },
      "outputs": [],
      "source": [
        "modelvgg19 = build_modelVGG19()\n",
        "modelvgg19.summary()\n",
        "history = modelvgg19.fit(train_images, training_labels, epochs=20, batch_size=64, validation_data=(val_images, validation_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OndVg9dJNqML"
      },
      "source": [
        "Se evalua con el dataset de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ITXYG0nNqML"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = modelvgg19.evaluate(X_t, test_labels)\n",
        "print(\"Test Loss: \", test_loss)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eld9lL24NqMM"
      },
      "source": [
        "Se crea la matriz de confusión y el classification report para ver como se han comportado cada una de las clases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vInmfv30NqMM"
      },
      "outputs": [],
      "source": [
        "class_names = [MAP_CHARACTERS[i] for i in range(len(MAP_CHARACTERS))]\n",
        "\n",
        "# Matriz de confusión y métricas adicionales\n",
        "Y_pred = modelvgg19.predict(X_t)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_t, y_pred, target_names=class_names))\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_t, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCpD_fQXNqMN"
      },
      "source": [
        "Ahora vamos con la parte de Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcEjwyHwNqMa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "classes_to_keep = [MAP_CHARACTERS[i] for i in range(len(MAP_CHARACTERS))]\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        rescale = 1./255,\n",
        "\trotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest',\n",
        "        validation_split=0.2)  # aquí es donde especificas la división\n",
        "\n",
        "# Generador para el entrenamiento\n",
        "train_generator = datagen.flow_from_directory(\n",
        "        '/root/.keras/datasets/simpsons',  # directorio que contiene las imágenes\n",
        "        target_size=(60, 60),\n",
        "        batch_size=64,\n",
        "        class_mode='categorical',\n",
        "        classes=classes_to_keep,\n",
        "        shuffle=True,\n",
        "        subset='training')  # especifica que estas imágenes son para entrenamiento\n",
        "\n",
        "# Generador para la validación\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "        '/root/.keras/datasets/simpsons',  # nota que usamos el mismo directorio que antes\n",
        "        target_size=(60, 60),\n",
        "        batch_size=64,\n",
        "        class_mode='categorical',\n",
        "        classes=classes_to_keep,\n",
        "        shuffle=False,\n",
        "        subset='validation')  # especifica que estas imágenes son para validación\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_3PEGjDNqMb"
      },
      "source": [
        "Adicionalmente y para que el Test Generator sea capaz de leer correctamente las imágenes de simpsons_testset, se proporciona este script que crea un directorio por clase y mueve cada imagen a su directorio correspondiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlCx1V9aNqMb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Directorio donde están todas las imágenes\n",
        "src_dir = '/root/.keras/datasets/simpsons_testset'\n",
        "\n",
        "# Obtén la lista de todas las imágenes\n",
        "image_files = os.listdir(src_dir)\n",
        "\n",
        "for img in image_files:\n",
        "    # Ignora los directorios, procesa solo archivos\n",
        "    if not os.path.isfile(os.path.join(src_dir, img)):\n",
        "        continue\n",
        "\n",
        "    # Obtén el nombre de la clase de la imagen (todo lo que precede al último '_')\n",
        "    class_name = '_'.join(img.split('_')[:-1])\n",
        "\n",
        "    # Define el nombre del subdirectorio\n",
        "    class_dir = os.path.join(src_dir, class_name)\n",
        "    \n",
        "    # Si el subdirectorio no existe, crea el subdirectorio\n",
        "    if not os.path.exists(class_dir):\n",
        "        os.mkdir(class_dir)\n",
        "    \n",
        "    # Define el nombre de archivo del destino\n",
        "    dst_file = os.path.join(class_dir, img)\n",
        "    \n",
        "    # Si el archivo de destino no existe, mueve la imagen al subdirectorio\n",
        "    if not os.path.exists(dst_file):\n",
        "        shutil.move(os.path.join(src_dir, img), dst_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlqctCyaNqMc"
      },
      "source": [
        "Se cargan las imagenes de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxRY0VXaNqMc"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)  \n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        '/root/.keras/datasets/simpsons_testset', \n",
        "        target_size=(60, 60),\n",
        "        batch_size=64,\n",
        "        class_mode='categorical',\n",
        "        classes=classes_to_keep,\n",
        "        shuffle=False) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyZN82Q6NqMd"
      },
      "source": [
        "Se entrena con el primer modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnZa3BiRNqMe"
      },
      "outputs": [],
      "source": [
        "model = build_modelVGG19()\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "h15=model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    validation_data=validation_generator)  # ajustado para usar todos los datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef-kqkMgNqMe"
      },
      "source": [
        "Se evalua con los datos de test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list all data in history\n",
        "print(h15.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(h15.history['accuracy'])\n",
        "plt.plot(h15.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(h15.history['loss'])\n",
        "plt.plot(h15.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y5MYIDKmbLFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8HaimuzNqMf"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(\"Test Loss: \", test_loss)\n",
        "print(\"Test Accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78qhAOJINqMg"
      },
      "source": [
        "Se crea la matriz de confusión y el classification report para ver como se han comportado cada una de las clases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2nuo2oSNqMh"
      },
      "outputs": [],
      "source": [
        "class_names = [MAP_CHARACTERS[i] for i in range(len(MAP_CHARACTERS))]\n",
        "\n",
        "#Etiquetas verdaderas\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Se hace la predicción\n",
        "Y_pred = model.predict(test_generator)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F15dbDIgNqMh"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Calcula la matriz de confusión\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Crea una figura y un conjunto de ejes\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "# Crea un mapa de calor a partir de la matriz de confusión\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", ax=ax, cmap=\"Blues\", cbar=False)\n",
        "\n",
        "# Añade las etiquetas a los ejes\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "\n",
        "# Añade las etiquetas de las clases a los ejes\n",
        "ax.set_xticklabels(class_names, rotation=90)  # Etiquetas en vertical en el eje X\n",
        "ax.set_yticklabels(class_names, rotation=0)   # Etiquetas en horizontal en el eje Y\n",
        "\n",
        "# Muestra la figura\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallbackFinal(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('val_accuracy')> 0.89):\n",
        "              print(\"\\nAlcanzado el 89% de precisión en datos de test, se cancela el entrenamiento!!\")\n",
        "              self.model.stop_training = True\n",
        "\n",
        "callbackFinal = myCallbackFinal()\n",
        "\n",
        "model_fully_connected= keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(60, 60, 3)),\n",
        "    keras.layers.Dense(150, activation='elu'),\n",
        "    keras.layers.Dense(18, activation='softmax')\n",
        "])\n",
        "\n",
        "model_fully_connected.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "history_final = model_fully_connected.fit(train_images, training_labels, epochs=40, batch_size=64, validation_data=(val_images, validation_labels), callbacks=[callbackFinal])\n"
      ],
      "metadata": {
        "id": "p_Hw26Plc3dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list all data in history\n",
        "print(history_final.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history_final.history['accuracy'])\n",
        "plt.plot(history_final.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history_final.history['loss'])\n",
        "plt.plot(history_final.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gnrXB4galPxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W2dLY0lEc9sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model_fully_connected.evaluate(X_t, test_labels, verbose=2)\n",
        "print('Precisión del modelo:', test_accuracy)"
      ],
      "metadata": {
        "id": "UDhQgw6Mc-CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvmoXyzViT6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Actividad_2_Redes_Neuronales_Convolucionales_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}